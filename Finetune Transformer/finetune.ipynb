{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: torch in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch scikit-learn pandas\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0               title  \\\n",
      "0        2379  Tipping the Velvet   \n",
      "1        5778        The Thin Man   \n",
      "2       36041       Saving Steele   \n",
      "3        9588  The Winthrop Woman   \n",
      "4       12657   Operation Redwood   \n",
      "\n",
      "                                              author  \\\n",
      "0                                       Sarah Waters   \n",
      "1                                   Dashiell Hammett   \n",
      "2                      Anne Jolin (Goodreads Author)   \n",
      "3  Anya Seton, Philippa Gregory (Goodreads Author...   \n",
      "4                                  S. Terrell French   \n",
      "\n",
      "                                         description  \\\n",
      "0  Nan King, an oyster girl, is captivated by the...   \n",
      "1  Nick and Nora Charles are Hammett's most encha...   \n",
      "2  Kennedy Cross had her heart broken early in li...   \n",
      "3  First published in 1958 and set in the early 1...   \n",
      "4  \"Sibley Carter is a moron and a worldclass jer...   \n",
      "\n",
      "                             genres  \n",
      "0                   fiction,romance  \n",
      "1                           fiction  \n",
      "2              romance,contemporary  \n",
      "3                   fiction,romance  \n",
      "4  fiction,young adult,contemporary  \n",
      "   Unnamed: 0                            title  \\\n",
      "0        9300  Love You to Death / High Stakes   \n",
      "1       33723            The Stonehenge Legacy   \n",
      "2       23437                    Lying in Wait   \n",
      "3       34556                         Fearless   \n",
      "4       13057                    Eternal Flame   \n",
      "\n",
      "                                author  \\\n",
      "0         Meg Cabot (Goodreads Author)   \n",
      "1                         Sam Christer   \n",
      "2        Liz Nugent (Goodreads Author)   \n",
      "3  Rachel Van Dyken (Goodreads Author)   \n",
      "4      Cynthia Eden (Goodreads Author)   \n",
      "\n",
      "                                         description  \\\n",
      "0  I can see ghosts. I can talk to ghosts. And, i...   \n",
      "1  Conspiracy thrillers have tackled Da Vinci, At...   \n",
      "2  From the author of the international bestselle...   \n",
      "3  When I met Kiersten, time stood still. It was ...   \n",
      "4  Cynthia Eden invites you into the world of the...   \n",
      "\n",
      "                                     genres  \n",
      "0       young adult,fantasy,romance,fiction  \n",
      "1                           fiction,fantasy  \n",
      "2                                   fiction  \n",
      "3  romance,contemporary,young adult,fiction  \n",
      "4                   romance,fantasy,fiction  \n",
      "   Unnamed: 0                                  title  \\\n",
      "0        3971                      The King of Torts   \n",
      "1       36604                 The Apprentice's Quest   \n",
      "2       15009              The Stolen Moon of Londor   \n",
      "3       26854  The Adventure of the Golden Pince-Nez   \n",
      "4        6730                                   Defy   \n",
      "\n",
      "                                            author  \\\n",
      "0                  John Grisham (Goodreads Author)   \n",
      "1                                      Erin Hunter   \n",
      "2                 A.P. Stephens (Goodreads Author)   \n",
      "3  Arthur Conan Doyle, David Ian Davies (Narrator)   \n",
      "4                Sara B. Larson (Goodreads Author)   \n",
      "\n",
      "                                         description  \\\n",
      "0  The office of the public defender is not known...   \n",
      "1  Erin Hunters #1 bestselling Warriors series co...   \n",
      "2  The era of peace among the elves, men, and dwa...   \n",
      "3  A crime with no apparent motive, the murder of...   \n",
      "4  Alexa Hollen is a fighter. Forced to disguise ...   \n",
      "\n",
      "                                genres  \n",
      "0                              fiction  \n",
      "1          fantasy,fiction,young adult  \n",
      "2                      fantasy,fiction  \n",
      "3                              fiction  \n",
      "4  fantasy,young adult,romance,fiction  \n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"../Data/Train_and_Test_dataset/train.csv\")\n",
    "test_data = pd.read_csv(\"../Data/Train_and_Test_dataset/test.csv\")\n",
    "val_data = pd.read_csv(\"../Data/Train_and_Test_dataset/val.csv\")\n",
    "\n",
    "# Ki·ªÉm tra d·ªØ li·ªáu\n",
    "print(train_data.head())\n",
    "print(test_data.head())\n",
    "print(val_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize description\n",
    "train_encodings = tokenizer(\n",
    "    list(train_data[\"description\"]),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=128\n",
    ")\n",
    "test_encodings = tokenizer(\n",
    "    list(test_data[\"description\"]),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=128\n",
    ")\n",
    "val_encodings = tokenizer(\n",
    "    list(val_data[\"description\"]),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ªë l∆∞·ª£ng nh√£n: 5\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Chuy·ªÉn nh√£n t·ª´ chu·ªói sang danh s√°ch\n",
    "train_data[\"genres\"] = train_data[\"genres\"].apply(lambda x: x.split(\",\") if isinstance(x, str) else [])\n",
    "test_data[\"genres\"] = test_data[\"genres\"].apply(lambda x: x.split(\",\") if isinstance(x, str) else [])\n",
    "val_data[\"genres\"] = val_data[\"genres\"].apply(lambda x: x.split(\",\") if isinstance(x, str) else [])\n",
    "\n",
    "# Binarize nh√£n\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels_binary = mlb.fit_transform(train_data[\"genres\"])\n",
    "test_labels_binary = mlb.transform(test_data[\"genres\"])\n",
    "val_labels_binary = mlb.transform(val_data[\"genres\"])\n",
    "\n",
    "print(f\"S·ªë l∆∞·ª£ng nh√£n: {len(mlb.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# T·∫°o Dataset\n",
    "train_dataset = BookDataset(train_encodings, train_labels_binary)\n",
    "test_dataset = BookDataset(test_encodings, test_labels_binary)\n",
    "val_dataset = BookDataset(val_encodings, val_labels_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# S·ªë nh√£n (t∆∞∆°ng ·ª©ng v·ªõi nh√£n trong d·ªØ li·ªáu)\n",
    "num_labels = len(mlb.classes_)\n",
    "\n",
    "# Load m√¥ h√¨nh BERT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# C·∫•u h√¨nh hu·∫•n luy·ªán\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,                # Regularization\n",
    "    logging_dir=\"./logs\", \n",
    "    load_best_model_at_end=True,      # Load checkpoint t·ªët nh·∫•t\n",
    "    metric_for_best_model=\"f1\",       # Ti√™u ch√≠ ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t\n",
    "    save_total_limit=1,\n",
    "    greater_is_better=True        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    # Tr√≠ch xu·∫•t logits v√† labels\n",
    "    logits, labels = pred\n",
    "    predictions = (logits > 0).astype(int)  # Chuy·ªÉn logits th√†nh nh√£n nh·ªã ph√¢n\n",
    "    \n",
    "    # T√≠nh c√°c ch·ªâ s·ªë\n",
    "    f1 = f1_score(labels, predictions, average=\"micro\")\n",
    "    precision = precision_score(labels, predictions, average=\"micro\")\n",
    "    recall = recall_score(labels, predictions, average=\"micro\")\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_8876\\2721217396.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3a9f0738fa4fddbc00c72b2b29a9cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4356, 'grad_norm': 1.643088936805725, 'learning_rate': 1.7568684658400197e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3693, 'grad_norm': 2.16227126121521, 'learning_rate': 1.513736931680039e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23052f0f60274992972faee0b7b85b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3438918888568878, 'eval_accuracy': 0.47489396634286496, 'eval_f1': 0.839430711610487, 'eval_precision': 0.8420799519086264, 'eval_recall': 0.8367980884109917, 'eval_runtime': 40.4859, 'eval_samples_per_second': 180.532, 'eval_steps_per_second': 11.288, 'epoch': 1.0}\n",
      "{'loss': 0.339, 'grad_norm': 2.034111738204956, 'learning_rate': 1.2706053975200586e-05, 'epoch': 1.09}\n",
      "{'loss': 0.2948, 'grad_norm': 2.755068302154541, 'learning_rate': 1.0274738633600778e-05, 'epoch': 1.46}\n",
      "{'loss': 0.2914, 'grad_norm': 3.7234046459198, 'learning_rate': 7.843423292000973e-06, 'epoch': 1.82}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051695ebf6ac4ef6942427f76ee28c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3337067663669586, 'eval_accuracy': 0.5003420440552743, 'eval_f1': 0.8499615589331125, 'eval_precision': 0.8415505328492797, 'eval_recall': 0.858542413381123, 'eval_runtime': 41.9401, 'eval_samples_per_second': 174.272, 'eval_steps_per_second': 10.896, 'epoch': 2.0}\n",
      "{'loss': 0.2623, 'grad_norm': 2.644906520843506, 'learning_rate': 5.412107950401167e-06, 'epoch': 2.19}\n",
      "{'loss': 0.2382, 'grad_norm': 2.9473319053649902, 'learning_rate': 2.9807926088013615e-06, 'epoch': 2.55}\n",
      "{'loss': 0.226, 'grad_norm': 3.286533832550049, 'learning_rate': 5.49477267201556e-07, 'epoch': 2.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f411c2763142078370155cee0d9259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3404996395111084, 'eval_accuracy': 0.5015734026542619, 'eval_f1': 0.8509075834295697, 'eval_precision': 0.843638071751512, 'eval_recall': 0.8583034647550777, 'eval_runtime': 41.9124, 'eval_samples_per_second': 174.388, 'eval_steps_per_second': 10.904, 'epoch': 3.0}\n",
      "{'train_runtime': 1487.4239, 'train_samples_per_second': 44.225, 'train_steps_per_second': 2.765, 'train_loss': 0.3046903343580948, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4113, training_loss=0.3046903343580948, metrics={'train_runtime': 1487.4239, 'train_samples_per_second': 44.225, 'train_steps_per_second': 2.765, 'total_flos': 4327043632356096.0, 'train_loss': 0.3046903343580948, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36c7414bdc8433e97d67590c684f0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_loss': 0.33609864115715027, 'eval_accuracy': 0.504993843207005, 'eval_f1': 0.8520479909044669, 'eval_precision': 0.8478118487645132, 'eval_recall': 0.8563266778927111, 'eval_runtime': 41.8179, 'eval_samples_per_second': 174.782, 'eval_steps_per_second': 10.928, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(\"Test Results:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6430b7cc2f4a6f955bfd1f8c89bcbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results: {'eval_loss': 0.3404996395111084, 'eval_accuracy': 0.5015734026542619, 'eval_f1': 0.8509075834295697, 'eval_precision': 0.843638071751512, 'eval_recall': 0.8583034647550777, 'eval_runtime': 41.6488, 'eval_samples_per_second': 175.491, 'eval_steps_per_second': 10.973, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "val_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "print(\"Validation Results:\", val_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bert_finetuned\\\\tokenizer_config.json',\n",
       " './bert_finetuned\\\\special_tokens_map.json',\n",
       " './bert_finetuned\\\\vocab.txt',\n",
       " './bert_finetuned\\\\added_tokens.json',\n",
       " './bert_finetuned\\\\tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L∆∞u m√¥ h√¨nh v√† tokenizer\n",
    "trainer.save_model(\"./bert_finetuned\")\n",
    "tokenizer.save_pretrained(\"./bert_finetuned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
